name: SEO Audit Tool

# This workflow allows you to manually trigger the audit from the GitHub Actions tab.
on:
  workflow_dispatch:
    inputs:
      audit_url:
        description: 'URL to audit'
        required: true
        default: 
      audit_level:
        description: 'Audit intensity (basic, standard)'
        required: true
        default: 'standard'
        type: choice
        options:
          - basic
          - standard
      audit_scope:
        description: 'Crawl scope (only_onpage, indexed_pages, full_300_pages)'
        required: true
        default: 'only_onpage'
        type: choice
        options:
          - only_onpage
          - indexed_pages
          - full_300_pages
      competitor_url:
        description: 'Optional: Competitor URL for comparison'
        required: false
        default: ''

jobs:
  seo_audit_job:
    runs-on: ubuntu-latest
    
    steps:
      - name: 1. Checkout repository code
        uses: actions/checkout@v4

      - name: 2. Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip' # Speeds up subsequent runs

      # ðŸš€ CRITICAL FIX: Install *minimal* system dependencies for Playwright and lxml
      # libnss3 is crucial for running Chromium in a container/headless environment.
      # Build dependencies for Scrapy/lxml are also included for a clean install.
      - name: 3. Install Core System Dependencies (Playwright & lxml build)
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            libnss3 libfontconfig libgbm-dev \
            libxml2-dev libxslt1-dev zlib1g-dev

      # ðŸš€ CRITICAL FIX: This step now reliably installs both Python packages AND the browser.
      # 'playwright install --with-deps' is the correct and robust command to download the browser binaries.
      - name: 4. Install Python dependencies and Playwright browser
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt --upgrade
          # The key fix: Installs the actual browser executable (Chromium/Firefox/WebKit)
          # This command resolves the 'Pages Crawled: 0' error.
          playwright install --with-deps 
          python -m textblob.download_corpora
          python -m nltk.downloader punkt
          
      # ================================================================

      - name: 5. Run SEO Audit (Handles JS pages)
        # Sets the environment variables needed by main.py
        env:
          AUDIT_URL: ${{ github.event.inputs.audit_url }}
          AUDIT_LEVEL: ${{ github.event.inputs.audit_level }}
          AUDIT_SCOPE: ${{ github.event.inputs.audit_scope }}
          COMPETITOR_URL: ${{ github.event.inputs.competitor_url }}
        run: python main.py

      - name: 6. Upload SEO Audit Reports
        uses: actions/upload-artifact@v4
        with:
          name: seo-audit-reports-${{ github.event.inputs.audit_level }}-${{ github.event.inputs.audit_scope }}
          path: reports/
          
