# .github/workflows/audit.yml
name: Automated SEO Audit

# 1. Define the trigger: Manual workflow dispatch
on:
  workflow_dispatch:
    inputs:
      audit_url:
        description: 'Target URL for the SEO audit (e.g., https://example.com)'
        required: true
        default: 'https://quotes.toscrape.com/'
      audit_level:
        description: 'Audit scope (standard or advanced)'
        required: true
        default: 'standard'
        type: choice
        options:
          - standard
          - advanced
      audit_scope:
        description: 'Crawl depth (only_onpage, indexed_pages, full_300_pages)'
        required: true
        default: 'only_onpage'
        type: choice
        options:
          - only_onpage
          - indexed_pages
          - full_300_pages
      competitor_url:
        description: 'Optional competitor URL for analysis'
        required: false

jobs:
  run_seo_audit:
    # Use a modern, stable Ubuntu runner
    runs-on: ubuntu-latest
    
    steps:
      - name: 1. Checkout Repository
        uses: actions/checkout@v4

      # 2. Set up Python environment
      - name: 2. Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip' # Cache dependencies for faster runs

      # 3. Install necessary system dependencies for Playwright
      - name: 3. Install Playwright system dependencies
        run: sudo apt-get update && sudo apt-get install -y libnss3 libatk1.0-0 libatk-bridge2.0-0 libcups2 libgbm-dev libxkbcommon-x11-0

      # 4. Install Python dependencies and Playwright browser binaries
      - name: 4. Install Python dependencies and Playwright browser
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # CRITICAL: Installs the actual browser executable (Chromium) required by Scrapy-Playwright
          playwright install chromium --with-deps 
          # Install NLP dependencies for keyword_analysis.py
          python -m textblob.download_corpora
          python -m nltk.downloader punkt
          
      # 5. Run SEO Audit (Handles JS pages)
      - name: 5. Run SEO Audit
        # Pass inputs as environment variables, which main.py expects
        env:
          AUDIT_URL: ${{ github.event.inputs.audit_url }}
          AUDIT_LEVEL: ${{ github.event.inputs.audit_level }}
          AUDIT_SCOPE: ${{ github.event.inputs.audit_scope }}
          COMPETITOR_URL: ${{ github.event.inputs.competitor_url }}
        run: python main.py

      # 6. Upload SEO Audit Reports
      - name: 6. Upload SEO Audit Reports
        uses: actions/upload-artifact@v4
        with:
          # Dynamic artifact name for easier identification
          name: seo-audit-reports-${{ github.event.inputs.audit_level }}-${{ github.event.inputs.audit_scope }}
          path: reports/
          # Ensure the artifact is created even if the run fails partially
          if-no-files-found: warn 
          
