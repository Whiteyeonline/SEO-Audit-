name: SEO Audit Tool

# This workflow allows you to manually trigger the audit from the GitHub Actions tab.
on:
  workflow_dispatch:
    inputs:
      audit_url:
        description: 'URL to audit'
        required: true
        default: 
      audit_level:
        description: 'Audit intensity (basic, standard, full)'
        required: true
        default: 'standard'
        type: choice
        options:
          - basic
          - standard
          - full
      audit_scope:
        description: 'Crawl scope (only_onpage, indexed_pages, full_300_pages)'
        required: true
        default: 'only_onpage'
        type: choice
        options:
          - only_onpage
          - indexed_pages
          - full_300_pages
      competitor_url:
        description: 'Optional: Competitor URL for comparison'
        required: false
        default: ''

jobs:
  seo_audit_job:
    runs-on: ubuntu-latest
    
    steps:
      - name: 1. Checkout repository code
        uses: actions/checkout@v4

      - name: 2. Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip' # Speeds up subsequent runs

      # This step installs the necessary Linux system libraries for Playwright (browser) and lxml (parser).
      - name: 3. Install necessary system dependencies for Playwright and lxml
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            libnss3 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 libxtst6 libxss1 libgbm-dev \
            libasound2-data libasound2-plugins libpulse0 libffi-dev libx264-dev

      # This step installs all Python packages from requirements.txt (including lxml, Scrapy-Playwright, etc.)
      # and then downloads the actual Chromium browser needed for the crawl.
      - name: 4. Install Python dependencies and Playwright browser
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt --upgrade
          # Installs the actual browser executable (Chromium) that Scrapy-Playwright uses
          playwright install --with-deps 
          python -m textblob.download_corpora
          python -m nltk.downloader punkt
          
      # ================================================================

      - name: 5. Run SEO Audit (Handles JS pages)
        # Sets the environment variables needed by main.py
        env:
          AUDIT_URL: ${{ github.event.inputs.audit_url }}
          AUDIT_LEVEL: ${{ github.event.inputs.audit_level }}
          AUDIT_SCOPE: ${{ github.event.inputs.audit_scope }}
          COMPETITOR_URL: ${{ github.event.inputs.competitor_url }}
        run: python main.py

      - name: 6. Upload SEO Audit Reports
        uses: actions/upload-artifact@v4
        with:
          name: seo-audit-reports-${{ github.event.inputs.audit_level }}-${{ github.event.inputs.audit_scope }}
          path: reports/
          
